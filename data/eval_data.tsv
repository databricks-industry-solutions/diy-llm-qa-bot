question	correct_source	answer
what’s the difference between spark cache and disk cache in databricks?	https://docs.databricks.com/optimizations/disk-cache.html	The main difference between Spark cache and disk cache in Databricks is that Spark cache stores data in memory, while disk cache stores data on local disk storage. Additionally, Spark cache can store the result of any subquery data and data stored in formats other than Parquet, while disk cache works for all Parquet data files (including Delta Lake tables). The disk cache can be read and operated on faster than the Spark cache because it uses efficient decompression algorithms and outputs data in the optimal format for further processing using whole-stage code generation.
what SQL command can I use to create a view with dynamic security for row access?	https://docs.databricks.com/data-governance/unity-catalog/create-views.html	To create a view with dynamic security for row access, you can use a dynamic view. In Unity Catalog, dynamic views can be used to provide row- and column-level access control, in addition to data masking. You can use the current_user() and is_account_group_member() functions to dynamically limit which users can access a row, column, or record in a view. To create a dynamic view with row-level permissions, you can use the CASE statement to filter out matching results for users who do not have permission to view the row. 
what are the benefits of lakehouse over a traditional data warehouse?	https://docs.databricks.com/migration/warehouse-to-lakehouse.html	The benefits of a lakehouse over a traditional data warehouse include combining the ACID transactions and data governance of enterprise data warehouses with the flexibility and cost-efficiency of data lakes, enabling business intelligence (BI) and machine learning (ML) on all data. The Databricks Lakehouse keeps data in massively scalable cloud object storage in open source data standards, allowing users to use their data however and wherever they want. Additionally, the data lakehouse replaces the current dependency on data lakes and data warehouses for modern data companies that desire open, direct access to data stored in standard data formats, indexing protocols optimized for machine learning and data science, and low query latency and high reliability for BI and advanced analytics.
Does Databricks support ANSI SQL?	https://docs.databricks.com/sql/language-manual/sql-ref-ansi-compliance.html	Yes, Databricks supports ANSI SQL. Databricks Runtime has two options to support compliance with the ANSI SQL standard: spark.sql.ansi.enabled and spark.sql.storeAssignmentPolicy. When spark.sql.ansi.enabled is set to true, Spark SQL uses an ANSI compliant dialect instead of being Hive compliant. For example, Spark will throw an exception at runtime instead of returning null results if the inputs to a SQL operator/function are invalid.
How to set a custom runtime version?	https://kb.databricks.com/clusters/run-a-custom-databricks-runtime-on-your-cluster	To set a custom runtime version, you need to follow the instructions provided in the article. You can use the workspace UI or API to set the custom image with the spark_version attribute when starting a cluster. The article provides step-by-step instructions for both methods. However, it is important to note that custom Databricks runtime images are created for specific, short-term fixes and edge cases, and should only be used if provided by Databricks Support during case resolution.
How to upload file to DBFS?	https://kb.databricks.com/dbfs/upload-large-files-using-dbfs-api-20-and-powershell	To upload large files to your Databricks workspace, you can use PowerShell and the DBFS API. The DBFS API 2.0 put command limits the amount of data that can be passed using the contents parameter to 1 MB if the data is passed as a string. The same command can pass 2 GB if the data is passed as a file. You can use curl or PowerShell to send a multipart form post request to the API to upload a file up to 2 GB in size. The PowerShell script is longer than the curl example, but it sends the same multipart form post request to the API.
How to delete a job?	https://docs.databricks.com/workflows/jobs/jobs.html	To delete a job, you can manually select jobs for removal from the workspace UI or use the Jobs Quota notebook to identify jobs which are not used very often and delete them. You can pass the job_id of the job you want to delete to the function deleteByJobId() in the Jobs Quota notebook. A confirmation message is shown when a job is successfully deleted.
how to schedule a job on my notebook?	https://docs.databricks.com/notebooks/schedule-notebook-jobs.html	To schedule a job on your notebook, you can click on the "Schedule" button at the top right of the notebook. If no jobs exist for this notebook, the Schedule dialog appears. If jobs already exist for the notebook, the Jobs List dialog appears. To display the Schedule dialog, click "Add a schedule". In the Schedule dialog, you can enter a name for the job, select "Manual" to run your job only when manually triggered, or "Scheduled" to define a schedule for running the job. If you select "Scheduled", use the drop-downs to specify the frequency, time, and time zone. In the Cluster drop-down, select the cluster to run the task. Optionally, enter any Parameters to pass to the job, specify email addresses to receive Alerts on job events, and click "Submit".
how to export the code of the notebook?	https://docs.databricks.com/notebooks/notebook-export-import.html	To export the code of a Databricks notebook, you can select "File > Export" in the notebook toolbar and select the export format. The supported formats include source file, HTML, Databricks .dbc archive, IPython notebook, and RMarkdown. If you choose to export as a source file, you will get a ZIP archive of notebook source files, which can be imported into a Databricks workspace, used in a CI/CD pipeline, or viewed as source files in each notebook’s default language. Notebook command outputs are not included.
how does the automatic termination  work for a cluster?	https://docs.databricks.com/clusters/clusters-manage.html	You can set auto termination for a cluster. During cluster creation, you can specify an inactivity period in minutes after which you want the cluster to terminate.  If the difference between the current time and the last command run on the cluster is more than the inactivity period specified, Databricks automatically terminates that cluster.  A cluster is considered inactive when all commands on the cluster, including Spark jobs, Structured Streaming, and JDBC calls, have finished executing. This does not include commands run by SSH-ing into the cluster and running bash commands.
how to copy s3 files to Databricks dbfs?	https://docs.databricks.com/storage/amazon-s3.html	To copy S3 files to Databricks DBFS, you can use the `dbutils.fs.cp()` command. Here is an example:  ``` dbutils.fs.cp("s3://my-bucket/path/to/file", "dbfs:/mnt/my-mount-point/path/to/destination") ```  You can also use the `dbutils.fs.cp()` command to copy an entire directory:  ``` dbutils.fs.cp("s3://my-bucket/path/to/directory", "dbfs:/mnt/my-mount-point/path/to/destination", True) ```  Note that you will need to have the appropriate credentials and permissions to access the S3 bucket.
how to feed parquet files to pytorch training?	https://docs.databricks.com/machine-learning/load-data/ddl-data.html	To feed Parquet files to PyTorch training, you can use Petastorm, an open source data access library that enables directly loading data stored in Apache Parquet format. Petastorm is recommended for Databricks and Apache Spark users because Parquet is the recommended data format. You can refer to the article "Load data using Petastorm" for more details.
How to migrate a table to UC?	https://docs.databricks.com/data-governance/unity-catalog/migrate.html	To migrate a table to Unity Catalog, you can use the Data Explorer upgrade wizard to copy complete schemas (databases) and multiple external tables from your default Hive metastore to the Unity Catalog metastore. Before you begin, you must have a storage credential with an IAM role that authorizes Unity Catalog to access the tables’ location path, an external location that references the storage credential you just created and the path to the data on your cloud tenant, and CREATE EXTERNAL TABLE permission on the external locations of the tables to be upgraded. Then, you can select hive_metastore as your catalog and select the schema (database) that you want to upgrade, click Upgrade at the top right of the schema detail view, select all of the tables that you want to upgrade and click Next, set the destination catalog and schema for each table, review the table configurations, and click Create Query for Upgrade. When the query is done, each table’s metadata has been copied from Hive metastore to Unity Catalog. These tables are marked as upgraded in the upgrade wizard.
How to load images into Delta?	https://docs.databricks.com/machine-learning/reference-solutions/images-etl-inference.html	To load images into Delta, Databricks recommends using the Auto Loader to ETL images into a Delta table. The Auto Loader helps with data management and automatically handles continuously arriving new images.
how to use huggingface on Databricks?	https://docs.databricks.com/machine-learning/reference-solutions/hugging-face-transformer.html	To use Hugging Face Transformers on Databricks, you can follow the steps outlined in the article "Use Hugging Face Transformers for natural language processing (NLP) model training and inference". The article provides example notebooks and code that demonstrate text classification with MLflow logging and fine-tuning a text classification model on a single GPU. Hugging Face Transformers pipelines encode best practices and have default models selected for different tasks, making it easy to get started. You can use Pandas UDFs to distribute the model for computation on a cluster, and you can monitor GPU performance by viewing the live ganglia metrics for a cluster. Finally, you can log Transformers pipelines models to MLflow using a custom pyfunc model.
How to create a Databricks service pincipal?	https://docs.databricks.com/dev-tools/service-principals.html	To create a Databricks service principal, you can use the SCIM API 2.0 (ServicePrincipals) for workspaces. You cannot use the Databricks user interface. To call this API, you can use tools such as curl or Postman, or you can use Terraform. Once you create a Databricks service principal, you can create a Databricks access token for it using the Token Management API 2.0. To create a Databricks service principal, you need to be a workspace admin and have a personal access token for your Databricks workspace user. You also need to follow some instructions that are provided in the context.
How to access the storage location of a UC table?	https://docs.databricks.com/data-governance/unity-catalog/index.html	To access the storage location of a UC table, you need to know the root storage location of the metastore that the table belongs to. Each metastore is configured with a root storage location in cloud storage, which is used by default for storing data for managed tables. You can optionally specify managed table storage locations at the catalog or schema levels, overriding the root storage location. Managed tables always use the Delta table format. External tables are tables whose data lifecycle and file layout are not managed by UC. Use external tables to register large amounts of existing data in UC, or if you require direct access to the data using tools outside of Databricks clusters or DBSQL warehouses. When you drop an external table, UC does not delete the underlying data.
I have my private pypi repository, how to install from it?	https://kb.databricks.com/clusters/install-private-pypi-repo	To install libraries from a private PyPI repository, you need to create an init script that authenticates and downloads the PyPI library from the private repository. First, create a directory to store the init script, then create the init script using the provided code. After that, install the init script as a cluster-scoped init script and restart the cluster. Finally, you can include the init script in a create-job.json file when using the Jobs API to start a job cluster.
how to find my workspace ID?	https://kb.databricks.com/administration/find-your-workspace-id	To find your workspace ID in Databricks, you can either look at the URL displayed in your browser's address bar after logging into your workspace, where the workspace ID is the numeric value represented by XXXXX after o= in the URL, or you can run spark.conf.get("spark.databricks.clusterUsageTags.clusterOwnerOrgId") in a Python or Scala cell within a notebook to get the workspace ID.
How to share data with another Databricks user who does not have access to your unity catalog?	https://docs.databricks.com/data-sharing/share-data-databricks.html	To access the tables and notebooks in a share, a metastore admin or privileged user must create a catalog from the share. Then that user or another user granted the appropriate privilege can give other users access to the catalog and objects in the catalog, just as they would any other catalogs, schemas, or tables registered in Unity Catalog, with the important distinction being that users can be granted only read access on objects in catalogs that are created from Delta Sharing shares.  Shared notebooks live at the catalog level, and any user with the USE CATALOG privilege on the catalog can access them.
How do I use GPUs on Databricks?	https://docs.databricks.com/clusters/gpu.html	To use GPUs on Databricks, you need to create a GPU-enabled cluster. This is similar to creating any Spark cluster, but you need to ensure that the Databricks Runtime Version is a GPU-enabled version, the Worker Type and Driver Type are GPU instance types, and the number of workers is set to zero for single-machine workflows without Spark. Databricks supports several GPU-accelerated instance types, including P2, P3, P4d, G4, and G5 instance type series. Databricks Runtime installs the NVIDIA driver and libraries required to use GPUs on Spark driver and worker instances, including the CUDA Toolkit, cuDNN, and NCCL. Databricks Runtime also supports GPU-aware scheduling from Apache Spark 3.0, which is preconfigured on GPU clusters. Finally, you can use Databricks Container Services on clusters with GPUs to create portable deep learning environments with customized libraries.
How to create pyspark dataframes from pandas dataframes with pandas 2.0.0?	https://docs.databricks.com/getting-started/dataframes-python.html	To create PySpark dataframes from Pandas dataframes with Pandas 2.0.0, you can use the from_pandas function provided by PySpark. This function creates a PySpark dataframe, series, or index from a Pandas dataframe, series, or index. You can pass a Pandas dataframe to this function and it will return a PySpark dataframe.
How to use Global Temp View as table for join in read_sql_query - DataBricks	https://kb.databricks.com/sql/global-temp-view-not-found	To use a Global Temp View as a table for join in read_sql_query, you need to use the qualified table name with the global_temp database. This is because all global temporary views are tied to a system temporary database named global_temp. So, you can query the global view data successfully by using the following syntax: %sql select * from global_temp.<global-view-name>;
How do you view all defined variables when using databricks notebooks?	https://docs.databricks.com/notebooks/notebooks-code.html	With Databricks Runtime 12.1 and above, you can directly observe current Python variables in the notebook UI.  To open the variable explorer, click the variable explorer icon in the right sidebar. The variable explorer opens, showing the value and data type, including shape, for each variable that is currently defined in the notebook.To filter the display, enter text into the search box. The list is automatically filtered as you type.  Variable values are automatically updated as you run notebook cells.
how do you integrate databricks and Fivetran?	https://docs.databricks.com/partners/ingestion/fivetran.html	To integrate Databricks and Fivetran, you can use Partner Connect or connect to Fivetran manually. Partner Connect does not integrate Fivetran with Databricks clusters. To integrate a cluster with Fivetran, connect to Fivetran manually. Before connecting to Fivetran manually, you need to have a cluster or SQL warehouse in your Databricks workspace, the connection details for your cluster or SQL warehouse, and a Databricks personal access token.
how do I get support?	https://docs.databricks.com/resources/support.html	To get support, you need to have a Databricks Support contract and your email address must be registered as an authorized support contact. If you have a Databricks support subscription, you can open and manage support cases. You can find answers to many questions on the Databricks Help Center if your organization does not have a Databricks support subscription. To view and submit support cases, you must log in to the Databricks Help Center. You can log in with your Databricks workspace account or your Databricks Support credentials. If you have any questions, you can submit a support case by emailing help@databricks.com.
How to create a Databricks-backed secret scope?	https://docs.databricks.com/security/secrets/secret-scopes.html	Secret scope names are case insensitive. To create a scope using the Databricks CLI:  Bash databricks secrets create-scope --scope <scope-name> By default, scopes are created with MANAGE permission for the user who created the scope. If your account does not have the Premium plan or above, you must override that default and explicitly grant the MANAGE permission to “users” (all users) when you create the scope:  Bash databricks secrets create-scope --scope <scope-name> --initial-manage-principal users You can also create a Databricks-backed secret scope using the Secrets API Put secret operation.  If your account has the Premium plan or above, you can change permissions at any time after you create the scope. For details, see Secret access control.  Once you have created a Databricks-backed secret scope, you can add secrets.
how to read a UC table	https://docs.databricks.com/data-governance/unity-catalog/index.html	To read a UC (Unity Catalog) table, you need to reference it using a three-level namespace (`catalog`.`schema`.`table`). The hierarchy of primary data objects flows from metastore to table, where metastore is the top-level container for metadata, catalog is the first layer of the object hierarchy used to organize data assets, schema is the second layer of the object hierarchy and contains tables and views, and table is at the lowest level in the object hierarchy. Tables can be managed or external, where managed tables are the default way to create tables in UC and UC manages the lifecycle and file layout for these tables, and external tables are tables whose data lifecycle and file layout are not managed by UC.
how do I provide Databricks engineers and Support staff temporary access to my workspace?	https://docs.databricks.com/administration-guide/workspace/workspace-access.html	To block Databricks access to your workspace, you can use a feature called Customer Approved Workspace Login (CAWL). This feature allows admins to give Databricks engineers and support staff access to their workspace for a temporary session. However, if needed, you can temporarily approve access to your workspace for only the duration of the support session. To enable this feature, you need to contact your Databricks representative to add an account-level setting. Once it is enabled, workspace admins can configure workspace access for their workspace by going to the Admin Console, clicking the Workspace Settings tab, and toggling the Workspace access for Databricks Support option. They can also set the amount of hours they would like to allow access to their workspace, and track the expiration time of access in the Workspace Settings tab.
how do I view the status of a Databricks service?	https://docs.databricks.com/resources/status.html	To view the status of a Databricks service, you can go to the Databricks Status Page, which provides an overview of all core Databricks services. You can easily view the status of a specific service by viewing the status page. The status page is broken down by AWS region. Select one of the three main geos (Americas, Europe, or Asia Pacific) to display all of the active regions in the selected geo. Service status is tracked on a per-region basis. It is possible for a service to have a different status across different regions.
how do I setup KMS encryption for S3?	https://docs.databricks.com/storage/amazon-s3.html	To set up KMS encryption for S3, you can encrypt your root S3 bucket using customer-managed keys, which requires using the Account API 2.0. You can either add an encryption key when you create a new workspace using the Account API or add the key later. For more information, see Customer-managed keys for workspace storage.
can I have multiple catalogs in a Unity Catalog metastore?	https://docs.databricks.com/data-governance/unity-catalog/best-practices.html	Yes, you can have multiple catalogs in a Unity Catalog metastore.
what is the difference between Unity Catalog shared and single-user cluster access modes?	https://docs.databricks.com/data-governance/unity-catalog/compute.html	Unity Catalog Single-User clusters can be assigned to and used by a single user. It supports Python, SQL, Scala and R. Unity Catalog Shared clusters can be used by multiple users with data isolation among users with some limitations. It supports SQL and Python.
how can I implement row level security in Databricks?	https://docs.databricks.com/administration-guide/access-control/table-acl.html	To implement row level security in Databricks, you can enable and enforce table access control for your workspace. This feature requires the Premium plan and above. Once enabled, you can restrict users to clusters with table access control enabled to ensure that they only access the data that you want them to. For more information on how to enable and enforce table access control, please refer to the context.
when does Databricks Runtime 11.3 reach end of support?	https://docs.databricks.com/release-notes/runtime/releases.html	Databricks Runtime 11.3 LTS will reach end-of-support on October 19, 2025.
What’s the difference between dbfs and UC?	https://docs.databricks.com/dbfs/unity-catalog.html	DBFS (Databricks File Store) is the default location for storing files associated with a number of actions performed in the Databricks workspace, including creating managed tables in the workspace-scoped hive_metastore. Actions performed against tables in the hive_metastore use legacy data access patterns, which may include data and storage credentials managed by DBFS. Unity Catalog introduces a number of new configurations and concepts that approach data governance entirely differently than DBFS. Databricks recommends against using DBFS and mounted cloud object storage for most use cases in Unity Catalog-enabled Databricks workspaces.
how to set an embedded metastore on a cluster?	https://kb.databricks.com/metastore/set-up-embedded-metastore	To set up an embedded Apache Hive metastore on a Databricks cluster, you can follow the instructions in the "Set up an embedded Hive metastore notebook". This notebook uses the Apache Derby embedded metastore, which is an in-memory lightweight database. However, it's important to note that an embedded metastore only retains table metadata during the life of the cluster, and if the cluster is restarted, the metadata is lost. If you need to persist the table metadata or other data after a cluster restart, then you should use the default metastore or set up an external metastore.